# Training configuration for Anime-Llasa-3B
model:
  name: "NandemoGHS/Anime-Llasa-3B"
  load_in_8bit: false
  load_in_4bit: false
  use_flash_attention: false
  
training:
  output_dir: "./output"
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-5
  warmup_steps: 100
  logging_steps: 10
  save_steps: 500
  eval_steps: 500
  save_total_limit: 3
  fp16: false
  bf16: false
  gradient_checkpointing: true
  max_grad_norm: 1.0
  
data:
  train_file: null
  validation_file: null
  max_length: 2048
  preprocessing_num_workers: 4
  
lora:
  enabled: false
  r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  target_modules:
    - q_proj
    - v_proj
  bias: "none"
  task_type: "CAUSAL_LM"
  
wandb:
  enabled: false
  project: "anime-llasa-3b"
  name: null
  entity: null

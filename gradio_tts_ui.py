"""
ã‚·ãƒ³ãƒ—ãƒ«ãªGradio TTS UI - LLASAã‚¯ãƒ©ã‚¹ä½¿ç”¨ç‰ˆ
"""

import gradio as gr
import os
import time
from modules.llasa_utils import normalize_text
import sys
import torch

# CUDAè¨­å®š
os.environ["CUDA_VISIBLE_DEVICES"] = "0"


def generate_speech(text: str, temperature: float = 0.7, top_p: float = 0.9, repeat_penalty: float = 1.0, max_tokens: int = 300):
    """æ™‚é–“è¨ˆæ¸¬ä»˜ãéŸ³å£°ç”Ÿæˆ"""
    start_time = time.time()
    
    audio_path, status, tokens = llasa.generate(text, temperature, top_p, repeat_penalty, max_tokens)
    
    elapsed_time = time.time() - start_time
    
    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã«æ™‚é–“æƒ…å ±ã‚’è¿½åŠ 
    if audio_path:
        status_with_time = f"âœ… ç”Ÿæˆå®Œäº†ï¼ â±ï¸ {elapsed_time:.2f}ç§’ {len(tokens.split())} tokens {len(tokens.split())/elapsed_time:.1f}t/s"
    else:
        status_with_time = f"âŒ {status} â±ï¸ {elapsed_time:.2f}ç§’"
    
    return audio_path, status_with_time, tokens

# Gradio UI
with gr.Blocks(title="LLASA TTS", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¤ LLASA-3B TTS")
    
    with gr.Row():
        with gr.Column(scale=2):
            text_input = gr.Textbox(
                label="ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›",
                value="",
                lines=3
            )

            normalized_text = gr.Textbox(
                label="æ­£è¦åŒ–ãƒ†ã‚­ã‚¹ãƒˆ",
                interactive=False
            )
            
            with gr.Row():
                temperature = gr.Slider(0.1, 2.0, 0.7, step=0.01, label="Temperature")
                top_p = gr.Slider(0.1, 1.0, 0.9, step=0.01, label="Top-p")
                repeat_penalty = gr.Slider(0.1, 2.0, 1.1, step=0.01, label="Repeat Penalty")
            
            max_tokens = gr.Slider(50, 2000, 500, step=25, label="æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°")
            generate_btn = gr.Button("ğŸµ éŸ³å£°ç”Ÿæˆ", variant="primary", size="lg")
        
        with gr.Column(scale=1):
            audio_output = gr.Audio(label="ç”ŸæˆéŸ³å£°", type="filepath")
            status_output = gr.Textbox(label="çŠ¶æ…‹", interactive=False)
            token_output = gr.Textbox(label="ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±", interactive=False, lines=10)
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
    samples = [
        "ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã¯ã„ã„å¤©æ°—ã§ã™ã­ã€‚",
        "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼",
        "é ‘å¼µã£ã¦ï¼å¿œæ´ã—ã¦ã‚‹ã‚ˆï¼",
        "ãŠã¤ã‹ã‚Œã•ã¾ã§ã—ãŸã€‚"
    ]
    
    with gr.Row():
        for sample in samples:
            btn = gr.Button(f"ã€Œ{sample}...ã€", size="sm")
            btn.click(lambda s=sample: s, outputs=text_input)
    
    # ã‚¤ãƒ™ãƒ³ãƒˆè¨­å®š
    generate_btn.click(
        generate_speech,
        inputs=[text_input, temperature, top_p, repeat_penalty, max_tokens],
        outputs=[audio_output, status_output, token_output]
    )

    text_input.change(
        normalize_text,
        inputs=text_input,
        outputs=normalized_text
    )
    
    text_input.submit(
        generate_speech,
        inputs=[text_input, temperature, top_p, repeat_penalty, max_tokens],
        outputs=[audio_output, status_output, token_output]
    )

if __name__ == "__main__":
    llasa_model = sys.argv[1] if len(sys.argv) > 1 else "./lora_checkpoints"
    vllm_mode = llasa_model == "vllm"
    
    # LLASAãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    if vllm_mode:
        from modules.llasa_server import LLASA
    else:
        from modules.llasa import LLASA
    
    llasa = LLASA.from_pretrained(llasa_model)
    if not vllm_mode:
        if hasattr(llasa.model, 'merge_and_unload'):
            llasa.model.merge_and_unload()
        llasa.model = torch.compile(llasa.model)
    demo.launch()
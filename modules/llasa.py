import torch
import tempfile
import soundfile as sf
from transformers import AutoTokenizer, AutoModelForCausalLM, Xcodec2Model, Xcodec2FeatureExtractor
from peft import AutoPeftModelForCausalLM
from modules.llasa_utils import get_prompt, SpeechOnlyProcessor


class LLASA:
    def __init__(self, model=None, tokenizer=None, codec_model=None, feature_extractor=None, compile_model=False):
        """LLASA-3B TTS ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
        
        Args:
            model: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆæ—¢ã«èª­ã¿è¾¼ã¿æ¸ˆã¿ã®å ´åˆï¼‰
            tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆæ—¢ã«èª­ã¿è¾¼ã¿æ¸ˆã¿ã®å ´åˆï¼‰
            codec_model: XCodec2ãƒ¢ãƒ‡ãƒ«ï¼ˆæ—¢ã«èª­ã¿è¾¼ã¿æ¸ˆã¿ã®å ´åˆï¼‰
            compile_model: torch.compile()ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ï¼ˆPyTorch 2.0+ï¼‰
        """
        
        self.model = model
        self.tokenizer = tokenizer
        self.codec_model = codec_model
        self.feature_extractor = feature_extractor
        self.device = model.device
        self.dtype = next(model.parameters()).dtype
        
        # torch.compile()ã«ã‚ˆã‚‹æœ€é©åŒ–ï¼ˆPyTorch 2.0+ï¼‰
        if compile_model and hasattr(torch, 'compile'):
            print("âš¡ torch.compile()ã§ãƒ¢ãƒ‡ãƒ«ã‚’æœ€é©åŒ–ä¸­...")
            self.model = torch.compile(self.model, mode="reduce-overhead")
            self.codec_model = torch.compile(self.codec_model, mode="reduce-overhead")
        
        self.logits_processor = SpeechOnlyProcessor(tokenizer=self.tokenizer, device=self.device, dtype=self.dtype)
        self.speech_start_id = self.tokenizer.convert_tokens_to_ids('<|s_0|>')
        self.speech_end_id = self.tokenizer.convert_tokens_to_ids('<|SPEECH_GENERATION_END|>')
        
        print("âœ… LLASA åˆæœŸåŒ–å®Œäº†ï¼")
    
    @classmethod
    def from_pretrained(cls, lora_path: str = "./lora_checkpoints", compile_model: bool = False, use_bf16: bool = False):
        """ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‹ã‚‰ LLASA ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
        
        Args:
            lora_path: ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
            compile_model: torch.compile()ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ï¼ˆPyTorch 2.0+ï¼‰
            use_bf16: bfloat16ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ï¼ˆA100ãªã©ã§é«˜é€ŸåŒ–ï¼‰
        """
        
        # ãƒ‡ãƒ¼ã‚¿å‹ã®é¸æŠ
        dtype = torch.bfloat16 if use_bf16 and torch.cuda.is_bf16_supported() else torch.float16
        print(f"ğŸ“Š ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿å‹: {dtype}")
        
        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        print("ğŸ“¦ LoRAãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
        try:
            model = AutoPeftModelForCausalLM.from_pretrained(
                lora_path,
                dtype=dtype,
                device_map="auto",
            ).eval()
        except:
            print("âš ï¸ é€šå¸¸ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦å†è©¦è¡Œä¸­...")
            model = AutoModelForCausalLM.from_pretrained(
                lora_path,
                dtype=dtype,
                device_map="auto",
            ).eval()
        
        print("ğŸ“ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ä¸­...")
        tokenizer = AutoTokenizer.from_pretrained(lora_path)
        
        print("ğŸµ XCodec2ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
        codec_model = Xcodec2Model.from_pretrained(
            "Anime-XCodec2-hf",
            torch_dtype=dtype,
        ).eval().to(model.device)
        feature_extractor = Xcodec2FeatureExtractor.from_pretrained("Anime-XCodec2-hf")
        
        return cls(model=model, tokenizer=tokenizer, codec_model=codec_model, feature_extractor=feature_extractor, compile_model=compile_model)
    
    @torch.no_grad()
    def generate(
        self,
        text: str,
        temperature: float = 0.7,
        top_p: float = 0.9,
        repeat_penalty: float = 1.1,
        max_tokens: int = 300,
    ) -> tuple[str, str, str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³å£°ã‚’ç”Ÿæˆ
        
        Returns:
            tuple[audio_path, status_msg, token_info]
        """
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        prompt = get_prompt(text)
        
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆãƒ‡ãƒã‚¤ã‚¹ã‚’è‡ªå‹•ã§åˆã‚ã›ã‚‹ï¼‰
        input_ids = self.tokenizer(prompt, return_tensors='pt').to(self.device)

        # éŸ³å£°ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ
        outputs = self.model.generate(
            **input_ids,
            max_new_tokens=max_tokens,
            eos_token_id=self.speech_end_id,
            do_sample=True,
            top_p=top_p,
            temperature=temperature,
            repetition_penalty=repeat_penalty,
            use_cache=True,
            pad_token_id=self.tokenizer.pad_token_id,
            logits_processor=[self.logits_processor],
        )
        
        # éŸ³å£°IDã‚’æŠ½å‡ºï¼ˆGPUä¸Šã§å‡¦ç†ã‚’ç¶™ç¶šï¼‰
        generated_ids = outputs[:, input_ids.input_ids.shape[1]:][0]
        speech_ids = []
        
        # ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚ŒãŸå‡¦ç†ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š
        mask = (generated_ids >= self.speech_start_id) & (generated_ids < self.speech_start_id + 65536)
        end_token_positions = (generated_ids == self.speech_end_id).nonzero(as_tuple=True)[0]
        
        if len(end_token_positions) > 0:
            end_pos = end_token_positions[0].item()
            mask[end_pos:] = False
        
        valid_tokens = generated_ids[mask]
        speech_ids = (valid_tokens - self.speech_start_id).tolist()
        
        if not speech_ids:
            return None, "âŒ æœ‰åŠ¹ãªéŸ³å£°ãƒˆãƒ¼ã‚¯ãƒ³ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ", ""
        
        # éŸ³å£°æ³¢å½¢ç”Ÿæˆï¼ˆtensoræ“ä½œã‚’æœ€å°é™ã«ï¼‰
        speech_codes = torch.tensor(speech_ids, dtype=torch.long, device=self.device).unsqueeze(0).unsqueeze(0)
        gen_wav = self.codec_model.decode(speech_codes).audio_values
        
        # CPUè»¢é€ã¯æœ€å¾Œã®1å›ã®ã¿
        gen_wav_cpu = gen_wav[0, 0, :].cpu().numpy()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
            sf.write(tmp_file.name, gen_wav_cpu, 16000)
            audio_path = tmp_file.name
        
        status_msg = f"âœ… ç”Ÿæˆå®Œäº† ({len(generated_ids)} tokens)"
        token_info = str(generated_ids.cpu().numpy().tolist())
        
        return audio_path, status_msg, token_info
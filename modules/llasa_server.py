import torch
import requests
from transformers import Xcodec2Model, Xcodec2FeatureExtractor
from modules.llasa_utils import get_prompt
from modules.llasa import BaseAudioDecoder

class LLASAServerClient(BaseAudioDecoder):
    def __init__(self, server_url="http://localhost:8000", codec_model=None, feature_extractor=None):
        """LLASA ã‚µãƒ¼ãƒãƒ¼ç‰ˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        
        Args:
            server_url: vLLMã‚µãƒ¼ãƒãƒ¼ã®URL
            codec_model: XCodec2ãƒ¢ãƒ‡ãƒ«ï¼ˆæ—¢ã«èª­ã¿è¾¼ã¿æ¸ˆã¿ã®å ´åˆï¼‰
            feature_extractor: XCodec2 feature extractorï¼ˆæ—¢ã«èª­ã¿è¾¼ã¿æ¸ˆã¿ã®å ´åˆï¼‰
        """
        
        super().__init__(codec_model, feature_extractor)
        self.server_url = server_url.rstrip('/')
        
        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³IDï¼ˆå›ºå®šå€¤ï¼‰
        self.speech_start_id = 128264  # <|s_0|>
        self.speech_end_id = 128261    # <|SPEECH_GENERATION_END|>
        
        print("âœ… LLASA ã‚µãƒ¼ãƒãƒ¼ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ åˆæœŸåŒ–å®Œäº†ï¼")
    
    @classmethod
    def from_pretrained(cls, model_path, server_url="http://localhost:8000"):
        """XCodec2ã¨ã‚µãƒ¼ãƒãƒ¼ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
        print("ğŸ”„ XCodec2ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        
        # XCodec2ã®èª­ã¿è¾¼ã¿
        try:
            codec_model = Xcodec2Model.from_pretrained("Anime-XCodec2-hf").eval().to('cuda:0')
            feature_extractor = Xcodec2FeatureExtractor.from_pretrained("Anime-XCodec2-hf")
            print("âœ… XCodec2ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
        except Exception as e:
            print(f"âŒ XCodec2èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
        
        # ã‚µãƒ¼ãƒãƒ¼ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        client = cls(
            server_url=server_url,
            codec_model=codec_model,
            feature_extractor=feature_extractor
        )
        
        print(f"âœ… LLASA ã‚µãƒ¼ãƒãƒ¼ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæº–å‚™å®Œäº† (vLLMã‚µãƒ¼ãƒãƒ¼: {server_url})")
        return client
    
    def _call_server(self, prompt: str, temperature: float, top_p: float, max_tokens: int, repeat_penalty: float):
        """ã‚µãƒ¼ãƒãƒ¼APIã‚’å‘¼ã³å‡ºã—"""
        
        data = {
            "llasa": "llasa",
            "prompt": prompt,
            "temperature": temperature,
            "top_p": top_p,
            "max_tokens": max_tokens,
            "repetition_penalty": repeat_penalty,
            "stop_token_ids": [self.speech_end_id],  # ç›´æ¥ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’æŒ‡å®š
            "stream": False
        }
        
        response = requests.post(
            f"{self.server_url}/v1/completions",
            headers={"Content-Type": "application/json"},
            json=data,
            timeout=120
        )
        
        if response.status_code == 200:
            result = response.json()
            return result["choices"][0]["text"], None
        else:
            return None, f"API ã‚¨ãƒ©ãƒ¼: {response}"
    
    @torch.no_grad()
    def generate_tokens(
        self,
        prompt: str,
        temperature: float = 0.7,
        top_p: float = 0.9,
        repeat_penalty: float = 1.1,
        max_tokens: int = 300,
    ) -> list[int]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³å£°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆï¼ˆã‚µãƒ¼ãƒãƒ¼ç‰ˆï¼‰
        
        Returns:
            list[int]: ç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ãƒˆãƒ¼ã‚¯ãƒ³åˆ—
        """
        
        # ã‚µãƒ¼ãƒãƒ¼ã§ç”Ÿæˆ
        generated_text, error = self._call_server(prompt, temperature, top_p, max_tokens, repeat_penalty)
        
        if error:
            raise RuntimeError(f"ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {error}")
        
        if not generated_text:
            raise RuntimeError("ç©ºã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒè¿”ã•ã‚Œã¾ã—ãŸ")
        
        # ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³å£°ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¤‰æ›
        # extract_speech_idsé–¢æ•°ã‚’å‚è€ƒã«ã—ãŸå®Ÿè£…
        try:
            from modules.llasa_utils import extract_speech_ids
            import re
            
            # éŸ³å£°ãƒˆãƒ¼ã‚¯ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ <|s_æ•°å€¤|> ã‚’æ¤œç´¢
            speech_token_pattern = r'<\|s_(\d+)\|>'
            matches = re.findall(speech_token_pattern, generated_text)
            
            if matches:
                # ãƒãƒƒãƒã—ãŸæ•°å€¤ã‚’æ•´æ•°ã«å¤‰æ›
                generated_ids = [int(match) for match in matches]
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç›´æ¥æ•°å€¤ã‚’æŠ½å‡º
                number_pattern = r'\d+'
                numbers = re.findall(number_pattern, generated_text)
                if numbers:
                    generated_ids = [int(num) for num in numbers[:max_tokens//2]]
                else:
                    # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é©å½“ãªéŸ³å£°IDç”Ÿæˆ
                    estimated_count = min(len(generated_text) // 10, max_tokens // 4, 100)
                    generated_ids = list(range(0, estimated_count))
                    
        except Exception as e:
            print(f"âš ï¸ éŸ³å£°IDæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            estimated_count = min(len(generated_text) // 10, max_tokens // 4, 50)
            generated_ids = list(range(0, estimated_count))
        
        # éŸ³å£°IDã‚’æ¤œè¨¼ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        speech_ids = []
        
        for speech_id in generated_ids:
            # éŸ³å£°IDã®ç¯„å›²ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆ0-65535ï¼‰
            if 0 <= speech_id < 65536:
                speech_ids.append(speech_id)
            else:
                print(f"âš ï¸ ç¯„å›²å¤–ã®éŸ³å£°ID: {speech_id}")
        
        return speech_ids

# å®Œå…¨äº’æ›ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
LLASA = LLASAServerClient
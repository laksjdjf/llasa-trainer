import torch
import requests
from transformers import Xcodec2Model, Xcodec2FeatureExtractor
from modules.llasa import BaseAudioDecoder
from modules.llasa_utils import extract_speech_ids

class LLASAServer(BaseAudioDecoder):
    def __init__(
            self,
            model="http://localhost:8000",
            tokenizer=None,
            codec_model=None,
            feature_extractor=None
        ):
        """LLASA ã‚µãƒ¼ãƒãƒ¼ç‰ˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        
        Args:
            server_url: vLLMã‚µãƒ¼ãƒãƒ¼ã®URL
            codec_model: XCodec2ãƒ¢ãƒ‡ãƒ«ï¼ˆæ—¢ã«èª­ã¿è¾¼ã¿æ¸ˆã¿ã®å ´åˆï¼‰
            feature_extractor: XCodec2 feature extractorï¼ˆæ—¢ã«èª­ã¿è¾¼ã¿æ¸ˆã¿ã®å ´åˆï¼‰
        """

        self.server_url = model.rstrip('/')
        super().__init__(model=None, tokenizer=tokenizer, codec_model=codec_model, feature_extractor=feature_extractor)
    
    @classmethod
    def from_pretrained(
        cls, 
        model_path="server",
        codec_model_path: str="Anime-XCodec2-hf",
        dtype=torch.float16,
    ):
        """XCodec2ã¨ã‚µãƒ¼ãƒãƒ¼ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
        print("ğŸ”„ XCodec2ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        
        # XCodec2ã®èª­ã¿è¾¼ã¿
        codec_model = Xcodec2Model.from_pretrained(codec_model_path, device_map="auto").eval()
        feature_extractor = Xcodec2FeatureExtractor.from_pretrained(codec_model_path)
        print("âœ… XCodec2ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
        
        # ã‚µãƒ¼ãƒãƒ¼ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        client = cls(
            model="http://localhost:8000",
            tokenizer=None,
            codec_model=codec_model,
            feature_extractor=feature_extractor
        )
        
        print(f"âœ… LLASA ã‚µãƒ¼ãƒãƒ¼ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæº–å‚™å®Œäº† (vLLMã‚µãƒ¼ãƒãƒ¼: {model_path})")
        return client
    
    def _call_server(self, prompt: str, temperature: float, top_p: float, repeat_penalty: float, max_tokens: int, min_tokens: int):
        """ã‚µãƒ¼ãƒãƒ¼APIã‚’å‘¼ã³å‡ºã—"""
        
        data = {
            "llasa": "llasa",
            "prompt": prompt,
            "temperature": temperature,
            "top_p": top_p,
            "max_tokens": max_tokens,
            "min_tokens": min_tokens,
            "repetition_penalty": repeat_penalty, # vllm
            "repeat_penalty": repeat_penalty, # llama.cpp
            "stop_token_ids": [self.speech_end_id],  # ç›´æ¥ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’æŒ‡å®š
            "stream": False
        }
        
        response = requests.post(
            f"{self.server_url}/v1/completions",
            headers={"Content-Type": "application/json"},
            json=data,
        )
        
        if response.status_code == 200:
            result = response.json()
            return result["choices"][0]["text"], None
        else:
            return None, f"API ã‚¨ãƒ©ãƒ¼: {response}"
    
    @torch.no_grad()
    def generate_tokens(
        self,
        prompt: str,
        temperature: float = 0.7,
        top_p: float = 0.9,
        repeat_penalty: float = 1.1,
        max_tokens: int = 300,
        min_tokens: int = 0,
    ) -> list[int]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³å£°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆï¼ˆã‚µãƒ¼ãƒãƒ¼ç‰ˆï¼‰
        
        Returns:
            list[int]: ç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ãƒˆãƒ¼ã‚¯ãƒ³åˆ—
        """
        
        # ã‚µãƒ¼ãƒãƒ¼ã§ç”Ÿæˆ
        if prompt.startswith("<|begin_of_text|>"):
            prompt = prompt[len("<|begin_of_text|>"):]
        generated_text, error = self._call_server(prompt, temperature, top_p, repeat_penalty, max_tokens, min_tokens)
        
        if error:
            raise RuntimeError(f"ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {error}")
        
        speech_ids = extract_speech_ids(generated_text)

        if not generated_text:
            raise RuntimeError("ç©ºã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒè¿”ã•ã‚Œã¾ã—ãŸ")
        
        return speech_ids

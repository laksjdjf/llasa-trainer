import os
from transformers import TrainingArguments
from peft import LoraConfig
from trl import SFTTrainer, DataCollatorForCompletionOnlyLM

# LLASAã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from modules.llasa import LLASA
from modules.train_utils import TTSTestCallback, load_dataset

def main(config):
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    # CUDAè¨­å®š
    os.environ["CUDA_VISIBLE_DEVICES"] = config.cuda_visible_devices

    # LoRAè¨­å®šï¼ˆnullã®å ´åˆã¯FFTã‚’ä½¿ç”¨ï¼‰
    if config.lora is not None:
        lora_config = LoraConfig(
            r=config.lora.r,
            lora_alpha=config.lora.lora_alpha,
            target_modules=list(config.lora.target_modules),
            lora_dropout=config.lora.lora_dropout,
            bias=config.lora.bias,
            task_type="CAUSAL_LM",
        )
        print(f"ğŸ”§ LoRAè¨­å®š: r={config.lora.r}, alpha={config.lora.lora_alpha}")
    else:
        lora_config = None
        print("ğŸ”§ FFT (Full Fine-tuning) ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨")
    
    # å­¦ç¿’è¨­å®šï¼ˆå‹•çš„ã«å¼•æ•°ã‚’å–å¾—ï¼‰
    training_kwargs = {
        "output_dir": config.output_dir,
    }
    
    # config.trainingã®å…¨ã¦ã®è¨­å®šã‚’å‹•çš„ã«è¿½åŠ 
    if hasattr(config, 'training') and config.training is not None:
        for key, value in config.training.items():
            training_kwargs[key] = value
            print(f"ğŸ”§ å­¦ç¿’è¨­å®š: {key} = {value}")
    
    training_args = TrainingArguments(**training_kwargs)
    
    # LLASAã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’æœ€åˆã«ä½œæˆï¼ˆXCodec2ã‚‚å«ã‚€ï¼‰
    print("ğŸ¯ LLASAã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆä¸­...")
    llasa = LLASA.from_pretrained(model_path=config.model_name, codec_model_path=config.get('codec_model_name', "Anime-XCodec2-hf"))

    collator = DataCollatorForCompletionOnlyLM(
        "<|SPEECH_GENERATION_START|>",
        tokenizer=llasa.tokenizer,
    )
    
    # ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆè¨­å®šãŒã‚ã‚Œã°ï¼‰
    callbacks = []
    if hasattr(config, 'test') and config.test is not None:
        test_callback = TTSTestCallback(
            llasa=llasa,
            test_text=config.test.text,
            test_interval=config.test.interval,
            save_path=os.path.join(config.output_dir, "samples")
        )
        callbacks.append(test_callback)
        print(f"ğŸ§ª ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š: interval={config.test.interval}")
    else:
        print("ğŸ§ª ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ãªã—")

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿
    train_dataset = load_dataset(config.data_dir)

    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
    trainer = SFTTrainer(
        model=llasa.model,
        tokenizer=llasa.tokenizer,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=collator,
        dataset_text_field="text",
        callbacks=callbacks,
        peft_config=lora_config,
    )

    # ã‚¹ãƒ†ãƒƒãƒ—0ã§ãƒ†ã‚¹ãƒˆç”Ÿæˆ
    if callbacks:
        print("\n--- åˆæœŸçŠ¶æ…‹ã§ã®ãƒ†ã‚¹ãƒˆç”Ÿæˆ ---")
        callbacks[0].test_generation(step=0)
        print("--- åˆæœŸãƒ†ã‚¹ãƒˆå®Œäº† ---\n")
    
    print("å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
    trainer.train()
    
    print("ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ä¸­...")
    trainer.save_model()

    # æœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—ã§ãƒ†ã‚¹ãƒˆç”Ÿæˆ
    if callbacks:
        print("\n--- æœ€çµ‚çŠ¶æ…‹ã§ã®ãƒ†ã‚¹ãƒˆç”Ÿæˆ ---")
        callbacks[0].test_generation(step='final')
        print("--- æœ€çµ‚ãƒ†ã‚¹ãƒˆå®Œäº† ---\n")
    
    print("å­¦ç¿’å®Œäº†ï¼")

if __name__ == "__main__":
    main()
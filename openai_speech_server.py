#!/usr/bin/env python3
"""
OpenAI Compatible Speech Generation Server for LLASA
OpenAI Speech APIäº’æ›ã®LLASAéŸ³å£°ç”Ÿæˆã‚µãƒ¼ãƒãƒ¼

Usage:
    python openai_speech_server.py --port 8001

API Endpoints:
    POST /v1/audio/speech - OpenAI Speech APIäº’æ›ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    GET /v1/models - åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§
"""

import argparse
import io
import time
import tempfile
import traceback
from typing import Optional, Literal
from pathlib import Path

import uvicorn
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel, Field
import soundfile as sf
import torch

from modules.llasa_server import LLASAServer
from modules.llasa_utils import normalize_text

# Pydantic models for OpenAI API compatibility
class SpeechRequest(BaseModel):
    model: str = Field(default="llasa-3b", description="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å")
    input: str = Field(..., description="éŸ³å£°ã«å¤‰æ›ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ")
    voice: str = Field(default="alloy", description="éŸ³å£°ã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆç¾åœ¨ã¯æœªä½¿ç”¨ï¼‰")
    response_format: Literal["mp3", "opus", "aac", "flac", "wav", "pcm"] = Field(default="mp3", description="éŸ³å£°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ")
    speed: float = Field(default=1.0, ge=0.25, le=4.0, description="å†ç”Ÿé€Ÿåº¦ï¼ˆç¾åœ¨ã¯æœªä½¿ç”¨ï¼‰")
    # LLASA specific parameters
    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description="ç”Ÿæˆã®å¤šæ§˜æ€§")
    top_p: float = Field(default=0.9, ge=0.0, le=1.0, description="Top-p sampling")
    repeat_penalty: float = Field(default=1.1, ge=0.0, le=2.0, description="ç¹°ã‚Šè¿”ã—ãƒšãƒŠãƒ«ãƒ†ã‚£")
    max_tokens: int = Field(default=300, ge=1, le=2048, description="æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    reference_text: Optional[str] = Field(default=None, description="å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆ")
    reference_audio: Optional[str] = Field(default=None, description="å‚ç…§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")

class ModelInfo(BaseModel):
    id: str
    object: str = "model"
    created: int
    owned_by: str = "llasa"

class ModelsResponse(BaseModel):
    object: str = "list"
    data: list[ModelInfo]

class ErrorResponse(BaseModel):
    error: dict

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
llasa_model: Optional[LLASAServer] = None

def initialize_model(model_path: str = "server", codec_model_path: str = "Anime-XCodec2-hf"):
    """LLASA ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–"""
    global llasa_model
    
    print(f"ğŸ”„ LLASA ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­... (codec: {codec_model_path})")
    try:
        llasa_model = LLASAServer.from_pretrained(
            model_path=model_path,
            codec_model_path=codec_model_path
        )
        print("âœ… LLASA ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†")
        return True
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return False

def audio_to_bytes(audio_data, sample_rate: int = 24000, format: str = "wav") -> bytes:
    """éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒã‚¤ãƒˆåˆ—ã«å¤‰æ›"""
    # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ– (-1.0 to 1.0)
    if torch.is_tensor(audio_data):
        audio_data = audio_data.cpu().numpy()
    
    # ãƒ¡ãƒ¢ãƒªä¸Šã®ãƒãƒƒãƒ•ã‚¡ã«éŸ³å£°ã‚’æ›¸ãè¾¼ã¿
    buffer = io.BytesIO()
    
    if format == "wav":
        sf.write(buffer, audio_data.T, sample_rate, format='WAV')
        content_type = "audio/wav"
    elif format == "mp3":
        # WAVã¨ã—ã¦ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¦ã‹ã‚‰MP3ã«å¤‰æ›ï¼ˆå®Ÿè£…ã¯ç°¡ç•¥åŒ–ï¼‰
        # å®Ÿéš›ã®æœ¬ç•ªç’°å¢ƒã§ã¯ ffmpeg ãªã©ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨
        sf.write(buffer, audio_data.T, sample_rate, format='WAV')
        content_type = "audio/mpeg"
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯WAV
        sf.write(buffer, audio_data.T, sample_rate, format='WAV')
        content_type = "audio/wav"
    
    buffer.seek(0)
    return buffer.getvalue(), content_type

# FastAPI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
app = FastAPI(
    title="LLASA OpenAI Compatible Speech API",
    description="OpenAI Speech APIäº’æ›ã®LLASAéŸ³å£°ç”Ÿæˆã‚µãƒ¼ãƒãƒ¼",
    version="1.0.0"
)

@app.get("/")
async def root():
    return {"message": "LLASA OpenAI Compatible Speech API Server", "status": "running"}

@app.get("/v1/models", response_model=ModelsResponse)
async def list_models():
    """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’è¿”ã™"""
    models = [
        ModelInfo(
            id="llasa-3b",
            created=int(time.time()),
            owned_by="llasa"
        )
    ]
    return ModelsResponse(data=models)

@app.post("/v1/audio/speech")
async def create_speech(request: SpeechRequest):
    """OpenAI Speech APIäº’æ›ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    
    if llasa_model is None:
        raise HTTPException(
            status_code=503,
            detail="Model not initialized. Please restart the server."
        )
    
    try:
        # ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†
        text = normalize_text(request.input)
        if not text.strip():
            raise HTTPException(
                status_code=400,
                detail="Input text is empty after normalization"
            )
        
        print(f"ğŸ¯ éŸ³å£°ç”Ÿæˆé–‹å§‹: '{text[:50]}{'...' if len(text) > 50 else ''}'")
        
        # LLASAã§éŸ³å£°ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ
        audio_path, speech_tokens = llasa_model.generate(
            text=text,
            max_tokens=4000,
        )
        
        if not audio_path:
            raise HTTPException(
                status_code=500,
                detail="Failed to generate speech tokens"
            )
        
        # éŸ³å£°ãƒ‡ã‚³ãƒ¼ãƒ‰
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
            # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            audio_data, sample_rate = sf.read(audio_path)
            
            # æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¿œã˜ã¦ãƒã‚¤ãƒˆåˆ—ã«å¤‰æ›
            audio_bytes, content_type = audio_to_bytes(
                audio_data, 
                sample_rate, 
                request.response_format
            )
            
            print(f"âœ… éŸ³å£°ç”Ÿæˆå®Œäº†: {len(speech_tokens)} tokens, {len(audio_bytes)} bytes")
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¨ã—ã¦è¿”ã™
            return StreamingResponse(
                io.BytesIO(audio_bytes),
                media_type=content_type,
                headers={
                    "Content-Disposition": f"attachment; filename=speech.{request.response_format}",
                    "X-Generated-Tokens": str(len(speech_tokens)),
                    "X-Audio-Duration": str(len(audio_data) / sample_rate)
                }
            )
    
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ éŸ³å£°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"Speech generation failed: {str(e)}"
        )

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """ã‚°ãƒ­ãƒ¼ãƒãƒ«ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    print(f"âŒ Unhandled exception: {exc}")
    traceback.print_exc()
    return JSONResponse(
        status_code=500,
        content=ErrorResponse(
            error={
                "message": f"Internal server error: {str(exc)}",
                "type": "internal_error",
                "code": "internal_error"
            }
        ).dict()
    )

def main():
    parser = argparse.ArgumentParser(description="LLASA OpenAI Compatible Speech API Server")
    parser.add_argument("--host", default="127.0.0.1", help="Host to bind to")
    parser.add_argument("--port", type=int, default=8001, help="Port to bind to")
    parser.add_argument("--model-path", default="server", help="LLASA model path")
    parser.add_argument("--codec-model-path", default="Anime-XCodec2-hf", help="XCodec2 model path")
    parser.add_argument("--workers", type=int, default=1, help="Number of worker processes")
    parser.add_argument("--cuda_visible_devices", type=str, default="0", help="ä½¿ç”¨ã™ã‚‹CUDAãƒ‡ãƒã‚¤ã‚¹ (ä¾‹: '0', '0,1')")
    
    args = parser.parse_args()

    import os
    os.environ["CUDA_VISIBLE_DEVICES"] = args.cuda_visible_devices
    
    print("ğŸš€ LLASA OpenAI Compatible Speech API Server ã‚’èµ·å‹•ä¸­...")
    print(f"   Host: {args.host}")
    print(f"   Port: {args.port}")
    print(f"   Model: {args.model_path}")
    print(f"   Codec: {args.codec_model_path}")
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    if not initialize_model(args.model_path, args.codec_model_path):
        print("âŒ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚çµ‚äº†ã—ã¾ã™ã€‚")
        return 1
    
    # ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        workers=args.workers,
        log_level="info"
    )
    
    return 0

if __name__ == "__main__":
    exit(main())
"""
LoRAã‚’ä½¿ã£ãŸLLASA-3Bãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
éŸ³ã‚¹ãƒˆâ†’ã‚¹ãƒ”ãƒ¼ãƒãƒˆãƒ¼ã‚¯ãƒ³ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
"""

import os
import torch
import numpy as np
from pathlib import Path
from typing import List, Dict, Any
from dataclasses import dataclass
import re

from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    TrainerCallback
)
from peft import (
    LoraConfig,
    TaskType,
    get_peft_model,
    prepare_model_for_kbit_training
)

# LLASAã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from llasa import LLASA
from xcodec2.modeling_xcodec2 import XCodec2Model

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§LLASAã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç®¡ç†
_global_llasa = None

class TTSTestCallback(TrainerCallback):
    """å­¦ç¿’ä¸­ã«TTSã®ãƒ†ã‚¹ãƒˆã‚’è¡Œã†ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    
    def __init__(self, test_text: str = "ã‚„ã£ã»ãƒ¼ï¼ãƒœã‚¯ã®åå‰ã¯ã‚¢ã‚¹ãƒˆãƒ«ãƒ•ã‚©ï¼", test_interval: int = 100):
        self.test_text = test_text  # æ­£è¦åŒ–ã¯LLASAã§è¡Œã†
        self.test_interval = test_interval
        self.step_count = 0
    
    def test_generation(self, model, tokenizer, step=None):
        """éŸ³å£°ç”Ÿæˆã‚’ãƒ†ã‚¹ãƒˆ"""
        try:
            global _global_llasa
            
            if _global_llasa is None:
                print("âŒ LLASAã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒã‚ã‚Šã¾ã›ã‚“")
                return
            
            # çŸ­ã„éŸ³å£°ç”Ÿæˆã§ãƒ†ã‚¹ãƒˆï¼ˆLLASAãŒè‡ªå‹•ã§ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–ï¼‰
            audio_path, status, tokens = _global_llasa.generate(
                self.test_text, 
                temperature=0.7, 
                top_p=0.9, 
                max_tokens=200  # çŸ­ã‚ã«ã—ã¦é«˜é€ŸåŒ–
            )
            
            if audio_path:
                print(f"ğŸµ ãƒ†ã‚¹ãƒˆç”ŸæˆæˆåŠŸ: '{self.test_text}' -> {status}")
                
                # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ï¼ˆã‚¹ãƒ†ãƒƒãƒ—ç•ªå·ä»˜ãï¼‰
                if step is not None:
                    save_path = f"./test_audio/step_{step}.wav"
                    os.makedirs("./test_audio", exist_ok=True)
                    import shutil
                    shutil.copy2(audio_path, save_path)
                    print(f"ğŸµ ãƒ†ã‚¹ãƒˆéŸ³å£°ä¿å­˜: {save_path}")
            else:
                print(f"âš ï¸ ãƒ†ã‚¹ãƒˆç”Ÿæˆå¤±æ•—: {status}")
                
        except Exception as e:
            print(f"âŒ ãƒ†ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        finally:
            model.train()
    
    def on_step_end(self, args, state, control, model, tokenizer=None, **kwargs):
        """ã‚¹ãƒ†ãƒƒãƒ—çµ‚äº†æ™‚ã«å‘¼ã°ã‚Œã‚‹"""
        self.step_count += 1
        
        if self.step_count % self.test_interval == 0 and tokenizer is not None:
            print(f"\n--- Step {state.global_step}: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ ---")
            self.test_generation(model, tokenizer, step=state.global_step)
            print("--- ãƒ†ã‚¹ãƒˆå®Œäº† ---\n")

@dataclass
class TTSDataCollator:
    """TTSç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼"""
    tokenizer: AutoTokenizer
    max_length: int = 2048

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        # ãƒãƒƒãƒå†…ã®æœ€å¤§é•·ã‚’å–å¾—
        max_len = min(max([len(f["input_ids"]) for f in features]), self.max_length)
        
        input_ids = []
        attention_mask = []
        labels = []
        
        for feature in features:
            ids = feature["input_ids"][:max_len]
            prompt_len = feature["prompt_length"]
            mask = [1] * len(ids)
            
            # labelsã‚’ä½œæˆ
            label_ids = ids.copy()
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ï¼ˆå­¦ç¿’å¯¾è±¡å¤–ï¼‰ã‚’-100ã§ãƒã‚¹ã‚¯
            for i in range(min(prompt_len, len(label_ids))):
                label_ids[i] = -100
            
            # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            padding_length = max_len - len(ids)
            ids.extend([self.tokenizer.pad_token_id] * padding_length)
            mask.extend([0] * padding_length)
            label_ids.extend([-100] * padding_length)  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°éƒ¨åˆ†ã‚‚lossã‹ã‚‰é™¤å¤–
            
            input_ids.append(ids)
            attention_mask.append(mask)
            labels.append(label_ids)
        
        return {
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "attention_mask": torch.tensor(attention_mask, dtype=torch.long),
            "labels": torch.tensor(labels, dtype=torch.long)
        }

class TTSDataset(Dataset):
    """éŸ³å£°åˆæˆç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    
    def __init__(self, data_dir: str, tokenizer: AutoTokenizer, max_length: int = 2048):
        self.data_dir = Path(data_dir)
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        self.data_files = []
        txt_files = sorted(list(self.data_dir.glob("*.txt")))
        
        for txt_file in txt_files:
            code_file = txt_file.with_name(txt_file.stem + "_codes.npy")
            if code_file.exists():
                self.data_files.append({
                    "text_file": txt_file,
                    "codes_file": code_file
                })
        
        print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã« {len(self.data_files)} å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
    
    def __len__(self):
        return len(self.data_files)
    
    def ids_to_speech_tokens(self, speech_ids):
        """éŸ³å£°IDã‚’éŸ³å£°ãƒˆãƒ¼ã‚¯ãƒ³æ–‡å­—åˆ—ã«å¤‰æ›"""
        speech_tokens_str = []
        for speech_id in speech_ids:
            speech_tokens_str.append(f"<|s_{speech_id}|>")
        return "".join(speech_tokens_str)
    
    def __getitem__(self, idx):
        data_info = self.data_files[idx]
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿
        with open(data_info["text_file"], "r", encoding="utf-8") as f:
            text = f.read().strip()
        
        # LLASAã®æ­£è¦åŒ–ã‚’ä½¿ç”¨ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«LLASAã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‹ã‚‰ï¼‰
        global _global_llasa
        if _global_llasa is not None:
            text = _global_llasa.normalize_text(text)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªæ­£è¦åŒ–
            text = text.strip()
        
        # éŸ³å£°ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿
        speech_codes = np.load(data_info["codes_file"])
        
        # éŸ³å£°ãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›
        speech_tokens = self.ids_to_speech_tokens(speech_codes)
        
        # ãƒãƒ£ãƒƒãƒˆå½¢å¼ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
        formatted_text = f"<|TEXT_UNDERSTANDING_START|>{text}<|TEXT_UNDERSTANDING_END|>"
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ï¼ˆå­¦ç¿’å¯¾è±¡å¤–ï¼‰
        chat_prompt = [
            {"role": "user", "content": "Convert the text to speech:" + formatted_text},
            {"role": "assistant", "content": f"<|SPEECH_GENERATION_START|>"}
        ]
        
        # å®Œå…¨ãªä¼šè©±ï¼ˆå­¦ç¿’å¯¾è±¡ã‚’å«ã‚€ï¼‰
        chat_full = [
            {"role": "user", "content": "Convert the text to speech:" + formatted_text},
            {"role": "assistant", "content": f"<|SPEECH_GENERATION_START|>{speech_tokens}<|SPEECH_GENERATION_END|>"}
        ]
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        prompt_ids = self.tokenizer.apply_chat_template(
            chat_prompt,
            tokenize=True,
            return_tensors=None,
            continue_final_message=True
        )
        
        # å®Œå…¨ãªä¼šè©±ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        full_ids = self.tokenizer.apply_chat_template(
            chat_full,
            tokenize=True,
            return_tensors=None,
            add_generation_prompt=False
        )
        
        # æœ€å¤§é•·ãƒã‚§ãƒƒã‚¯
        if len(full_ids) > self.max_length:
            full_ids = full_ids[:self.max_length]
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ãŒåˆ‡ã‚Šå–ã‚‰ã‚Œã‚‹å ´åˆã¯èª¿æ•´
            if len(prompt_ids) > self.max_length:
                prompt_ids = prompt_ids[:self.max_length]
        
        return {
            "input_ids": full_ids,
            "prompt_length": len(prompt_ids)  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·ã‚’ä¿å­˜
        }



def setup_model_and_tokenizer(model_name: str = "NandemoGHS/Anime-Llasa-3B"):
    """ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’è¨­å®š"""
    print(f"ãƒ¢ãƒ‡ãƒ« '{model_name}' ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # ãƒ¢ãƒ‡ãƒ«
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        trust_remote_code=True
    )
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’æ˜ç¤ºçš„ã«cuda:0ã«ç§»å‹•
    model = model.to('cuda:0')
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’æ˜ç¤ºçš„ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
    model.train()
    
    return model, tokenizer

def setup_lora_model(model, lora_config: LoraConfig):
    """LoRAã‚’é©ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š"""
    print("LoRAã‚’é©ç”¨ä¸­...")
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š
    model.train()
    
    # å‹¾é…ã‚’æœ‰åŠ¹ã«ã™ã‚‹
    for param in model.parameters():
        param.requires_grad = False
    
    # LoRAã‚’é©ç”¨
    model = get_peft_model(model, lora_config)
    
    # å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’è¡¨ç¤º
    model.print_trainable_parameters()
    
    return model

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    # CUDA:0ã®ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«è¨­å®š
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"
    
    # torchã§ã‚‚ãƒ‡ãƒã‚¤ã‚¹è¨­å®šã‚’ç¢ºå®Ÿã«ã™ã‚‹
    if torch.cuda.is_available():
        torch.cuda.set_device(0)
        print(f"CUDA_VISIBLE_DEVICES=0 ã«è¨­å®šã€ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {torch.cuda.current_device()}")
        print(f"åˆ©ç”¨å¯èƒ½ãªGPUæ•°: {torch.cuda.device_count()}")
    else:
        print("CUDAãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
    
    # è¨­å®š
    DATA_DIR = "dataset/kama1"
    OUTPUT_DIR = "./kama1_lora"
    MODEL_NAME = "NandemoGHS/Anime-Llasa-3B"
    
    # LoRAè¨­å®š
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=16,  # ãƒ©ãƒ³ã‚¯
        lora_alpha=32,  # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        lora_dropout=0.05,
        bias="none",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # åŸºæœ¬çš„ãªattentionãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã¿
    )
    
    # å­¦ç¿’è¨­å®š
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        overwrite_output_dir=True,
        num_train_epochs=20,
        per_device_train_batch_size=1,  # å°ã•ãã—ã¦å®‰å…¨ã«
        gradient_accumulation_steps=8,  # å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ç¶­æŒ
        learning_rate=1e-4,  # å°‘ã—å°ã•ã‚ã«
        weight_decay=0.01,
        logging_steps=10,
        save_steps=100,
        save_total_limit=3,
        prediction_loss_only=True,
        remove_unused_columns=False,
        dataloader_pin_memory=False,
        fp16=True,  # ãƒ¡ãƒ¢ãƒªç¯€ç´„
        gradient_checkpointing=False,  # ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚æœ‰åŠ¹åŒ–
        warmup_ratio=0.03,
        lr_scheduler_type="cosine",
        report_to=None,  # wandbãªã©ã‚’ä½¿ã†å ´åˆã¯ã“ã“ã‚’å¤‰æ›´
        ddp_find_unused_parameters=False,  # è¿½åŠ 
        dataloader_num_workers=0,  # ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚·ãƒ³ã‚°ç„¡åŠ¹åŒ–ã§CUDAã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå•é¡Œå›é¿
    )
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’è¨­å®š
    model, tokenizer = setup_model_and_tokenizer(MODEL_NAME)
    
    # LLASAã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’æœ€åˆã«ä½œæˆï¼ˆXCodec2ã‚‚å«ã‚€ï¼‰
    print("ğŸ¯ LLASAã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆä¸­...")
    global _global_llasa
    
    # XCodec2ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
    print("ğŸ“¦ XCodec2ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
    codec_model = XCodec2Model.from_pretrained(
        "NandemoGHS/Anime-XCodec2",
        torch_dtype=torch.float32
    ).eval().to('cuda:0')
    
    # LLASAã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
    _global_llasa = LLASA(model=model, tokenizer=tokenizer, codec_model=codec_model)
    print("âœ… LLASAã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆå®Œäº†")
    
    # LoRAã‚’é©ç”¨
    model = setup_lora_model(model, lora_config)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    train_dataset = TTSDataset(DATA_DIR, tokenizer)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼
    data_collator = TTSDataCollator(tokenizer)
    
    print(f"è¨“ç·´å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç¢ºèªä¸­...")
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"è¨“ç·´å¯èƒ½: {trainable_params:,} / å…¨ä½“: {total_params:,} ({100 * trainable_params / total_params:.2f}%)")
    
    # ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    test_callback = TTSTestCallback(
        test_text="ã“ã‚“ã«ã¡ã¯ã€ãƒã‚¹ã‚¿ãƒ¼ã•ã‚“ã€‚",
        test_interval=50  # 50ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒ†ã‚¹ãƒˆ
    )
    
    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ï¼ˆéŸ³å£°ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶ç´„ã¯LLASAãŒæ‹…å½“ï¼‰
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
        callbacks=[test_callback],
    )
    
    # å­¦ç¿’å‰ã®åˆæœŸãƒ†ã‚¹ãƒˆ
    print("\n--- å­¦ç¿’å‰ã®åˆæœŸãƒ†ã‚¹ãƒˆ ---")
    test_callback.test_generation(model, tokenizer, step="initial")
    print("--- åˆæœŸãƒ†ã‚¹ãƒˆå®Œäº† ---\n")
    
    # å­¦ç¿’é–‹å§‹
    print("å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
    trainer.train()
    
    # å­¦ç¿’å¾Œã®æœ€çµ‚ãƒ†ã‚¹ãƒˆ
    print("\n--- å­¦ç¿’å¾Œã®æœ€çµ‚ãƒ†ã‚¹ãƒˆ ---")
    test_callback.test_generation(model, tokenizer, step="final")
    print("--- æœ€çµ‚ãƒ†ã‚¹ãƒˆå®Œäº† ---\n")
    
    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    print("ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ä¸­...")
    trainer.save_model()
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    print("å­¦ç¿’å®Œäº†ï¼")

if __name__ == "__main__":
    main()